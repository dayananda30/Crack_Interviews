In this one we'll create a Work Queue that will be used to distribute time-consuming tasks among multiple workers.

The main idea behind Work Queues (aka: Task Queues) is to avoid doing a resource-intensive task immediately and
having to wait for it to complete. Instead we schedule the task to be done later. We encapsulate a task as a message
and send it to the queue. A worker process running in the background will pop the tasks and eventually execute the job.
When you run many workers the tasks will be shared between them.

We don't have a real-world task, like images to be resized or pdf files to be rendered,
so let's fake it by just pretending we're busy - by using the time.sleep() function.


Round-robin dispatching:
========================
One of the advantages of using a Task Queue is the ability to easily parallelise work.
If we are building up a backlog of work, we can just add more workers and that way, scale easily.

Message acknowledgment:
=======================
Doing a task can take a few seconds. You may wonder what happens if one of the consumers starts a long task and
dies with it only partly done. With our current code once RabbitMQ delivers message to the consumer it immediately
marks it for deletion. In this case, if you kill a worker we will lose the message it was just processing.
We'll also lose all the messages that were dispatched to this particular worker but were not yet handled.

But we don't want to lose any tasks. If a worker dies, we'd like the task to be delivered to another worker.

In order to make sure a message is never lost, RabbitMQ supports message acknowledgments. An ack(nowledgement)
is sent back by the consumer to tell RabbitMQ that a particular message had been received, processed and that RabbitMQ is free to delete it.

Manual message acknowledgments are turned on by default.
auto_ack


Message durability:
===================
When RabbitMQ quits or crashes it will forget the queues and messages unless you tell it not to.
Two things are required to make sure that messages aren't lost: we need to mark both the queue and messages as durable.
First, we need to make sure that the queue will survive a RabbitMQ node restart. In order to do so, we need to declare it as durable:

channel.queue_declare(queue='hello', durable=True)

This queue_declare change needs to be applied to both the producer and consumer code.

At that point we're sure that the task_queue queue won't be lost even if RabbitMQ restarts.
Now we need to mark our messages as persistent - by supplying a delivery_mode property with the value of pika.spec.PERSISTENT_DELIVERY_MODE

channel.basic_publish(exchange='',
                      routing_key="task_queue",
                      body=message,
                      properties=pika.BasicProperties(
                         delivery_mode = pika.spec.PERSISTENT_DELIVERY_MODE
                      ))


channel.basic_qos(prefetch_count=1) ==> to tell RabbitMQ not to give more than one message to a worker at a time





